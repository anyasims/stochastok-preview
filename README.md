<h1 align="center">StochasTok: Improving Fine-Grained Subword Understanding in LLMs</h1>

<p align="center">
    <em>Paper preview.<br>
      ArXiv version, code, and new results coming soon!</em>
</p>

# tl;dr

<div align="center">

**Instead of standard tokenization... use stochastic tokenization!**

<img src="assets/fig2.png" width="60%" />

&nbsp;

**There is no compromise to original performance.**

<img src="assets/fig3.png" width="60%" />

&nbsp;

**It dramatically improves downstream performance on language game tasks**

<img src="assets/fig1.png" width="60%" />

&nbsp;

**StochasTok-trained models grok multi-digit addition**

<img src="assets/fig8.png" width="60%" />

&nbsp;

**And it visibly alters LLMs' internal representations**

<img src="assets/fig12.png" width="60%" /></div>

&nbsp;

***For more results please see our paper!***

- StochasTok can be applied without pretraining from scratch,
- Is robust to hyperparameter choice,
- Has OOD generalization properties,
- Scales to larger models,
- and more...

# Citation

```
@misc{sims2025stochastok,
      title={StochasTok: Improving Fine-Grained Subword Understanding in LLMs}, 
      author={Anya Sims and Thom Foster and Klara Kaleb and Jakob N. Foerster and Yee Whye Teh and Cong Lu},
      year={2025},
}
```
