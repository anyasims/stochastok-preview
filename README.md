<h1 align="center">StochasTok: Improving Fine-Grained Subword Understanding in LLMs</h1>

<p align="center">
    <em>Paper preview.<br>
      ArXiv version, code, and new results coming soon!</em>
</p>

# tl;dr

**Instead of standard tokenization... use stochastic tokenization!**

<div align="center">
<img src="assets/fig2.png" width="60%" />

**There is no compromise to original performace.**

<div align="center">
<img src="assets/fig3.png" width="60%" />

**It dramatically improves dramatically downstream performace on language game tasks**

<div align="center">
<img src="assets/fig1.png" width="60%" />

**StochasTok-trained models grok multi-digit addition**

<div align="center">
<img src="assets/fig8.png" width="60%" />

**And it visibly alters LLMs' internal representions**

<div align="center">
<img src="assets/fig12.png" width="60%" />

***For more results please see our paper!***

- StochasTok can be applied without pretraining from scratch,
- Is robust to hyperparameter setting,
- Has OOD generalization properties,
- Scales to larger models,
- and more...

# Citation

```
@misc{sims2025stochastok,
      title={StochasTok: Improving Fine-Grained Subword Understanding in LLMs}, 
      author={Anya Sims and Thom Foster and Klara Kaleb and Jakob N. Foerster and Yee Whye Teh and Cong Lu},
      year={2025},
}
```
